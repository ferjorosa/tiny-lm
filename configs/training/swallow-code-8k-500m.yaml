# Precision and loss masking
precision: "bf16-mixed"
ignore_index: -100

# Optimizer and training stability
weight_decay: 0.1
betas: [0.9, 0.95]
eps: 1.0e-8
grad_clip_norm: 1.0

# Learning-rate schedule
learning_rate: 3.0e-4
scheduler: "cosine"
warmup_ratio: 0.01
min_lr: 6.0e-5

# Tokens, steps, and batching
# total batch size = batch_size * seq_len * accumulate_grad_batches * num_devices
# example: 64 * 1024 * 4 * 1 =  262144 tokens per update
# 500M tokens / 262144 tokens per update = 1907 steps
batch_size: 64
accumulate_grad_batches: 4
max_steps: 1907
val_every_n_steps: 50
system_metrics_every_n_steps: 5

# Checkpointing
save_every_n_steps: 100
resume_from_checkpoint: null
