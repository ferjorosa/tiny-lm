{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17474af572a0fc4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:21.298577Z",
     "start_time": "2024-12-09T17:55:59.989488Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, Qwen2Config, Qwen2ForCausalLM, Qwen2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9880ae995fefa",
   "metadata": {},
   "source": "# Load data"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13802a548b974b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:23.924159Z",
     "start_time": "2024-12-09T17:56:21.322291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 4\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset in streaming mode\n",
    "stories_stream = load_dataset(\n",
    "    \"roneneldan/TinyStories\", streaming=True, trust_remote_code=True\n",
    ")\n",
    "stories_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977f99d006615c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:26.338748Z",
     "start_time": "2024-12-09T17:56:24.265586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = 1100\n",
    "\n",
    "# Get the first 100 rows\n",
    "rows = list(stories_stream[\"train\"].take(n_rows))\n",
    "\n",
    "# Count the total number of characters\n",
    "total_chars = sum(len(row[\"text\"]) for row in rows)\n",
    "total_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392cb35751f3621d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:26.363286Z",
     "start_time": "2024-12-09T17:56:26.343722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "stories = Dataset.from_list(rows)\n",
    "\n",
    "print(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b16b9c250ad95ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:08.993604Z",
     "start_time": "2024-11-30T08:50:08.990118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3194dbbb94bb136f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:26.370936Z",
     "start_time": "2024-12-09T17:56:26.368323Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in tqdm(range(0, len(stories), batch_size)):\n",
    "        yield stories[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851c79cd10b1e18",
   "metadata": {},
   "source": "# Train Tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8688eae39836a14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:26.676597Z",
     "start_time": "2024-12-09T17:56:26.381479Z"
    }
   },
   "outputs": [],
   "source": [
    "base_tokenizer = Qwen2TokenizerFast.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\", errors=\"ignore\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a59bd82892ad6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:56:26.683823Z",
     "start_time": "2024-12-09T17:56:26.681212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' �'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer.decode([51461], errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f569b198e7f04743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T17:58:10.618105Z",
     "start_time": "2024-12-09T17:58:10.373084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 422.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1024\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b7be310f9fb50",
   "metadata": {},
   "source": "# Test untrained model"
  },
  {
   "cell_type": "markdown",
   "id": "ad5eb0b5c81c4950",
   "metadata": {},
   "source": "## Initialize model"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4fd74421ca7947f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.784925Z",
     "start_time": "2024-11-30T08:50:09.781751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 1024\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "config = Qwen2Config(\n",
    "    num_hidden_layers=3,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size * 4,  # MLP hidden dim, following GPT-2 approach x4\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=2,  # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=512,  # Maximum sequence length\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "456dba1060c3b4c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.817958Z",
     "start_time": "2024-11-30T08:50:09.799734Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Qwen2ForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de9a72e416747101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.834861Z",
     "start_time": "2024-11-30T08:50:09.832730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 309,984\n",
      "Total size of the model: 1.18 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f0110366ebb96",
   "metadata": {},
   "source": "## Generate text from the model"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a057ffc91dde7302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.851061Z",
     "start_time": "2024-11-30T08:50:09.849117Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_tokens_to_dataframe(tokenizer, inputs):\n",
    "    \"\"\"Decodes token IDs to tokens and returns them as a transposed DataFrame.\"\"\"\n",
    "    decoded_tokens = [tokenizer.decode(token_id) for token_id in inputs[0]]\n",
    "    token_ids = inputs[0].tolist()\n",
    "\n",
    "    # Create and return a transposed DataFrame\n",
    "    df = pd.DataFrame({\"Token\": decoded_tokens, \"Token ID\": token_ids})\n",
    "\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1be3e7548940ddfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.865981Z",
     "start_time": "2024-11-30T08:50:09.863912Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"One day a little girl, wakanda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "591e96099b3edbd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.890362Z",
     "start_time": "2024-11-30T08:50:09.880454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1    2        3      4   5    6   7    8   9\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a\n",
       "Token ID  446   371  272      406    451  25  283  88  711  78"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens_to_dataframe(tokenizer, inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2c34dca9c60cd80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.966780Z",
     "start_time": "2024-11-30T08:50:09.926260Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs[\"input_ids\"], max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ffbbc542101f0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:09.998129Z",
     "start_time": "2024-11-30T08:50:09.990525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>Timmy</td>\n",
       "      <td>Timmy</td>\n",
       "      <td>Timmy</td>\n",
       "      <td>Timmy</td>\n",
       "      <td>Timmy</td>\n",
       "      <td>Timmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1    2        3      4   5    6   7    8   9       10  \\\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a   Timmy   \n",
       "Token ID  446   371  272      406    451  25  283  88  711  78     546   \n",
       "\n",
       "              11      12      13      14      15  \n",
       "Token      Timmy   Timmy   Timmy   Timmy   Timmy  \n",
       "Token ID     546     546     546     546     546  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens_to_dataframe(tokenizer, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79f4eeb7838fba9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:10.058601Z",
     "start_time": "2024-11-30T08:50:10.055840Z"
    }
   },
   "outputs": [],
   "source": [
    "token_to_id = tokenizer.get_vocab()\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ccb00839f049",
   "metadata": {},
   "source": "> From [Qwen/tokenization_note.md](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md): The regular tokens are BPE tokens learned from byte sequences of texts encoded using the UTF-8 encoding. While this allows tokenization of all texts and no unknown token exists, it may fall back to using single bytes when tokenizing uncommon texts. You may encounter UTF-8 decoding errors and as the errors are default to replace, thus the replacement character (�) in incomplete generation."
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d25a32e087b0e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:10.099489Z",
     "start_time": "2024-11-30T08:50:10.096759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ó\n",
      "�\n",
      "ó\n"
     ]
    }
   ],
   "source": [
    "print(id_to_token[189])\n",
    "print(tokenizer.decode(189))\n",
    "print(tokenizer.convert_ids_to_tokens(189))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c24a68905a65083f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:10.164939Z",
     "start_time": "2024-11-30T08:50:10.162497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġt\n",
      " t\n",
      "Ġt\n"
     ]
    }
   ],
   "source": [
    "print(id_to_token[271])\n",
    "print(tokenizer.decode(271))\n",
    "print(tokenizer.convert_ids_to_tokens(271))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4a55ba24573c5",
   "metadata": {},
   "source": "# Train language model"
  },
  {
   "cell_type": "markdown",
   "id": "3ac9ccb01e1708f9",
   "metadata": {},
   "source": [
    "## Create Lightning Data Module\n",
    "\n",
    "In this case, we are creating a simple Lightning module when we have already loaded the Dataset. We would normally want to use this in a more complex manner, like with an iterableDataset from a series of Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0208ba94b9df4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:10.202350Z",
     "start_time": "2024-11-30T08:50:10.196869Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        n_train_rows: int,\n",
    "        n_val_rows: int,\n",
    "        batch_size: int,\n",
    "        max_seq_length: int,\n",
    "        num_workers: int,\n",
    "        tokenizer: Qwen2TokenizerFast,\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset_name: Name of the dataset.\n",
    "        :param n_train_rows: Number of training rows.\n",
    "        :param n_val_rows: Number of validation rows.\n",
    "        :param batch_size: Batch size.\n",
    "        :param max_seq_length: Max sequence length.\n",
    "        :param num_workers: Number of workers.\n",
    "        :param random_seed: Random seed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.n_train_rows = n_train_rows\n",
    "        self.n_val_rows = n_val_rows\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Load dataset in streaming mode\n",
    "        ds = load_dataset(self.dataset_name, streaming=True, trust_remote_code=True)\n",
    "\n",
    "        # Create dataset\n",
    "        self.train_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"train\",\n",
    "            n_rows=self.n_train_rows,\n",
    "        )\n",
    "        self.val_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"validation\",\n",
    "            n_rows=self.n_val_rows,\n",
    "        )\n",
    "\n",
    "        # Tokenizer\n",
    "        # TODO: In reality, we would initialize the tokenizer here\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def _create_dataset(self, ds, split, n_rows):\n",
    "        # Download and load N rows\n",
    "        rows = list(stories_stream[split].take(n_rows))\n",
    "        return Dataset.from_list(rows)\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "        # Extract text from batch\n",
    "        batch_text = [item[\"text\"] for item in batch]\n",
    "\n",
    "        # Tokenize texts\n",
    "        batch_tokenized = self.tokenizer(\n",
    "            batch_text,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Prepare labels by shifting input_ids\n",
    "        labels = batch_tokenized[\"input_ids\"].clone()\n",
    "        labels[:, :-1] = batch_tokenized[\"input_ids\"][:, 1:]\n",
    "        labels[:, -1] = self.tokenizer.pad_token_id\n",
    "\n",
    "        # Add labels to the returned dictionary\n",
    "        batch_tokenized[\"labels\"] = labels\n",
    "\n",
    "        return batch_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656842cb385e09aa",
   "metadata": {},
   "source": "### Test the `DataModule`"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c359a5af3a412b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:13.633486Z",
     "start_time": "2024-11-30T08:50:10.238248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[446, 371,  25, 272, 406, 451, 572, 398, 616, 272, 831, 322, 333, 326,\n",
      "         758,  27, 325, 789, 318, 294, 291, 602,  83, 477,  98,  89,  97, 280,\n",
      "         377, 353, 318, 883, 318, 294, 391, 301,  93,  27, 398, 461, 280, 902,\n",
      "         276, 831, 322, 353, 326, 399,  25, 356, 348, 480, 430, 100, 272, 433,\n",
      "          97, 299, 361, 326, 391, 327,  97, 319, 370, 491, 280, 326, 399, 278,\n",
      "         343,  25, 350, 810,  25, 346, 616, 745, 831, 322,  27, 880, 314, 366,\n",
      "         902, 318, 353, 516, 278, 430, 100, 625, 391, 327,  97, 610, 900, 399,\n",
      "         508, 278, 343,  25, 350, 922,  25, 398,  25, 373, 481, 902, 276, 831,\n",
      "         322, 278, 966, 101, 637, 391, 327,  97, 503,  65,  92, 576,  25, 380,\n",
      "         391, 606],\n",
      "        [455, 475, 272, 420,  25, 424, 294, 272, 406, 569, 572, 382,  82, 580,\n",
      "          27, 382,  82, 580, 560, 280, 449, 848, 278, 377, 333, 276, 749,  27,\n",
      "         382,  82, 580, 294, 272, 292, 416,  97,  85, 102, 569, 883, 292, 683,\n",
      "         375, 614, 285,  98, 379,  27, 913, 499, 285,  98, 379, 573, 382,  82,\n",
      "         580, 427, 278, 984, 319, 446, 371,  25, 382,  82, 580, 294, 291, 365,\n",
      "         777, 333, 276, 633, 593, 292, 434, 272, 425, 738,  27, 309, 738, 375,\n",
      "         785, 505,  78, 796, 372, 438, 285, 403, 303,  27, 382,  82, 580, 638,\n",
      "         766, 276, 505,  78, 796, 285, 403, 278, 461, 280, 377, 353, 452,  27,\n",
      "         382,  82, 580, 291, 649, 354, 958, 276, 738, 278, 739, 889, 276, 505,\n",
      "          78, 796]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[371,  25, 272, 406, 451, 572, 398, 616, 272, 831, 322, 333, 326, 758,\n",
      "          27, 325, 789, 318, 294, 291, 602,  83, 477,  98,  89,  97, 280, 377,\n",
      "         353, 318, 883, 318, 294, 391, 301,  93,  27, 398, 461, 280, 902, 276,\n",
      "         831, 322, 353, 326, 399,  25, 356, 348, 480, 430, 100, 272, 433,  97,\n",
      "         299, 361, 326, 391, 327,  97, 319, 370, 491, 280, 326, 399, 278, 343,\n",
      "          25, 350, 810,  25, 346, 616, 745, 831, 322,  27, 880, 314, 366, 902,\n",
      "         318, 353, 516, 278, 430, 100, 625, 391, 327,  97, 610, 900, 399, 508,\n",
      "         278, 343,  25, 350, 922,  25, 398,  25, 373, 481, 902, 276, 831, 322,\n",
      "         278, 966, 101, 637, 391, 327,  97, 503,  65,  92, 576,  25, 380, 391,\n",
      "         606,   0],\n",
      "        [475, 272, 420,  25, 424, 294, 272, 406, 569, 572, 382,  82, 580,  27,\n",
      "         382,  82, 580, 560, 280, 449, 848, 278, 377, 333, 276, 749,  27, 382,\n",
      "          82, 580, 294, 272, 292, 416,  97,  85, 102, 569, 883, 292, 683, 375,\n",
      "         614, 285,  98, 379,  27, 913, 499, 285,  98, 379, 573, 382,  82, 580,\n",
      "         427, 278, 984, 319, 446, 371,  25, 382,  82, 580, 294, 291, 365, 777,\n",
      "         333, 276, 633, 593, 292, 434, 272, 425, 738,  27, 309, 738, 375, 785,\n",
      "         505,  78, 796, 372, 438, 285, 403, 303,  27, 382,  82, 580, 638, 766,\n",
      "         276, 505,  78, 796, 285, 403, 278, 461, 280, 377, 353, 452,  27, 382,\n",
      "          82, 580, 291, 649, 354, 958, 276, 738, 278, 739, 889, 276, 505,  78,\n",
      "         796,   0]])}\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule(\n",
    "    dataset_name=\"roneneldan/TinyStories\",  # The dataset name\n",
    "    n_train_rows=10,  # For testing, load only 10 rows\n",
    "    n_val_rows=10,  # For testing, load only 10 validation rows\n",
    "    batch_size=2,  # Smaller batch size for testing\n",
    "    max_seq_length=128,  # Choose a max sequence length (e.g., 128)\n",
    "    num_workers=0,  # No need for multiple workers in a test scenario\n",
    "    tokenizer=tokenizer,  # Pass the tokenizer here\n",
    "    random_seed=42,  # Optional, for reproducibility\n",
    ")\n",
    "\n",
    "# Set up the data module\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Get the first batch from the train dataloader\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "\n",
    "# Iterate over the dataloader to get the first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the first batch to inspect it\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed047626d7c594",
   "metadata": {},
   "source": [
    "## Create ModelModule\n",
    "\n",
    "#### Shifting Inputs and Labels\n",
    "\n",
    "In language model pretraining, the goal is to predict the next token given the previous tokens. This is achieved by shifting the input and labels:\n",
    "\n",
    "- **Labels Shift**: `labels[..., 1:]` means \"take all dimensions, but slice off the first token from the last dimension.\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **Original input**: `[1, 2, 3, 4]`\n",
    "- **Shifted input**: `[2, 3, 4]`\n",
    "\n",
    "This creates an offset, ensuring that each input token predicts the next token.\n",
    "\n",
    "#### Why Shifting?\n",
    "\n",
    "In language modeling, the objective is to predict the next token. Therefore, the input `[1, 2, 3]` should predict the next tokens `[2, 3, 4]`. This is distinct from tasks like classification, where labels match exactly.\n",
    "\n",
    "**Example to Illustrate:**\n",
    "\n",
    "- **Input**: `[start, \"I\", \"love\", \"machine\"]`\n",
    "- **Labels**: `[\"I\", \"love\", \"machine\", \"learning\"]`\n",
    "- **Shifted Input**: `[\"I\", \"love\", \"machine\"]`\n",
    "- **Shifted Labels**: `[\"love\", \"machine\", \"learning\"]`\n",
    "\n",
    "#### Loss Computation\n",
    "\n",
    "The `CrossEntropyLoss` combines `log softmax` and `NLL (Negative Log Likelihood)` loss. The `reduction='mean'` option averages the loss across all tokens.\n",
    "\n",
    "- **Logits Transformation**: `logits.view(-1, logits.size(-1))` flattens the logits to a 2D tensor of shape `(batch_size * sequence_length, vocab_size)`.\n",
    "- **Labels Transformation**: `shift_labels.view(-1)` flattens the labels to a 1D tensor of shape `(batch_size * sequence_length)`.\n",
    "\n",
    "This setup ensures that the model is trained to predict the next token in the sequence accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8da0a8c8a12e2bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:13.668172Z",
     "start_time": "2024-11-30T08:50:13.663406Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Make it general, not focused around Qwen\n",
    "\n",
    "\n",
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        qwen_model_config: dict,\n",
    "        learning_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Qwen2ForCausalLM(qwen_model_config)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        )\n",
    "        return outputs.logits\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Shift labels for next-token prediction\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_input_ids = input_ids[..., 1:].contiguous()\n",
    "        shift_attention_mask = attention_mask[..., 1:].contiguous()\n",
    "\n",
    "        # Get logits\n",
    "        logits = self(shift_input_ids, shift_attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=50,  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        return generated_ids\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box usually works fine\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57ebd9d15e59cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:13.680322Z",
     "start_time": "2024-11-30T08:50:13.671629Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "\n",
    "config = Qwen2Config(\n",
    "    num_hidden_layers=3,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size * 4,  # MLP hidden dim, following GPT-2 approach x4\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=2,  # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=512,  # Maximum sequence length\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "model_module = ModelModule(\n",
    "    qwen_model_config=config,\n",
    "    learning_rate=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfa5ded8883759fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:13.700053Z",
     "start_time": "2024-11-30T08:50:13.697126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelModule(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(1024, 64)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (k_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=64, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45801af8da08837",
   "metadata": {},
   "source": "## Set up Trainer"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76f8c89622cd2681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:50:13.733807Z",
     "start_time": "2024-11-30T08:50:13.731151Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_trainer():\n",
    "    # Set up callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./model_checkpoints\",  # Directory to save checkpoints\n",
    "        filename=\"qwen-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=3,  # Save the top 3 models according to the monitored quantity\n",
    "        verbose=True,\n",
    "        monitor=\"val_loss\",  # Metric to monitor\n",
    "        mode=\"min\",  # We want to minimize the validation loss\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.00,\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=False,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    # Set up logger\n",
    "    logger = TensorBoardLogger(save_dir=\"./tb_logs\", name=\"qwen-pretrain-logs\")\n",
    "\n",
    "    # Create the trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,  # Number of training epochs\n",
    "        accelerator=\"auto\",  # Automatically use GPU if available\n",
    "        devices=1,  # Use 1 GPU or CPU\n",
    "        precision=\"16-mixed\",  # Use mixed precision training\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        logger=logger,\n",
    "        fast_dev_run=False,  # Set to True for a quick test run\n",
    "        gradient_clip_val=1.0,  # Gradient clipping\n",
    "        deterministic=True,  # For reproducibility\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "86a87c9f76b6887c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:52:22.093377Z",
     "start_time": "2024-11-30T08:52:19.486632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Create model and data modules (as defined in previous context)\n",
    "model_module = ModelModule(\n",
    "    qwen_model_config=config,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset_name=\"roneneldan/TinyStories\",\n",
    "    n_train_rows=256,\n",
    "    n_val_rows=128,\n",
    "    batch_size=16,\n",
    "    max_seq_length=128,\n",
    "    num_workers=0,\n",
    "    tokenizer=tokenizer,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = setup_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c92129d8ded941",
   "metadata": {},
   "source": "## Run training"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b178ad67ad2e1806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T08:52:40.312013Z",
     "start_time": "2024-11-30T08:52:29.453546Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | Qwen2ForCausalLM | 309 K  | train\n",
      "---------------------------------------------------\n",
      "309 K     Trainable params\n",
      "0         Non-trainable params\n",
      "309 K     Total params\n",
      "1.240     Total estimated model params size (MB)\n",
      "49        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a67fed290c40abbc80933f1c3fa001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernando/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b724ce7e5f434baef1e2c92344e7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7266ca069487690905b47490c6606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 16: 'val_loss' reached 6.91907 (best 6.91907), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=00-val_loss=6.92.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2752ed9671434a15a5218790eaf0146f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 32: 'val_loss' reached 6.90322 (best 6.90322), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=01-val_loss=6.90.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f9b9ce75db485a9e24e17b25e6ec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 48: 'val_loss' reached 6.88753 (best 6.88753), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=02-val_loss=6.89.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b2830cb43d4964a1c464cfd53a5072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 64: 'val_loss' reached 6.87166 (best 6.87166), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=03-val_loss=6.87.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb65e8ac25940b58da41b6172340344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 80: 'val_loss' reached 6.85561 (best 6.85561), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=04-val_loss=6.86.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b25508f4ba4e2195dfe4fa11dad974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 96: 'val_loss' reached 6.83963 (best 6.83963), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=05-val_loss=6.84.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3780a1af534d738a34418e9a9292f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 112: 'val_loss' reached 6.82397 (best 6.82397), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=06-val_loss=6.82.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fe9febf3484a31b3586f5dda01d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 128: 'val_loss' reached 6.80882 (best 6.80882), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=07-val_loss=6.81.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d5637156314e209ffb04e85f8391ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 144: 'val_loss' reached 6.79426 (best 6.79426), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=08-val_loss=6.79.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de84b2ee0d4a28b1dd1c51040e7820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 160: 'val_loss' reached 6.78031 (best 6.78031), saving model to '/Users/fernando/Documents/GitHub/tiny-lm/examples/model_checkpoints/qwen-epoch=09-val_loss=6.78.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model=model_module, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
