learning_rate: 3.0e-4
weight_decay: 0.1
betas: [0.9, 0.95]
eps: 1.0e-8
ignore_index: -100
precision: "bf16-mixed"
accumulate_grad_batches: 1
grad_clip_norm: 1.0
scheduler: "cosine"
warmup_ratio: 0.01
max_steps: 55000
# total batch size = batch_size * seq_len * accumulate_grad_batches * num_devices
# example: 8 * 1024 * 1 * 1 = 8192 tokens per update
# 450M tokens / 8192 tokens per update = 55000 steps
min_lr: 6.0e-5
save_every_n_steps: 1000
save_top_k: 3
val_every_n_steps: 1000
system_metrics_every_n_steps: 10
resume_from_checkpoint: null
