{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:31.113725Z",
     "start_time": "2024-11-29T18:25:28.687617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import Qwen2Config, Qwen2TokenizerFast, Qwen2ForCausalLM\n",
    "from transformers import AdamW\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ],
   "id": "17474af572a0fc4a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "f3a9880ae995fefa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:33.859848Z",
     "start_time": "2024-11-29T18:25:31.118407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset in streaming mode\n",
    "stories_stream = load_dataset('roneneldan/TinyStories', streaming=True, trust_remote_code=True)\n",
    "stories_stream"
   ],
   "id": "13802a548b974b13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 4\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.601227Z",
     "start_time": "2024-11-29T18:25:33.996507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_rows = 1100\n",
    "\n",
    "# Get the first 100 rows\n",
    "rows = list(stories_stream['train'].take(n_rows))\n",
    "\n",
    "# Count the total number of characters\n",
    "total_chars = sum(len(row[\"text\"]) for row in rows)\n",
    "total_chars"
   ],
   "id": "977f99d006615c3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.616172Z",
     "start_time": "2024-11-29T18:25:34.607055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stories = Dataset.from_list(rows)\n",
    "\n",
    "print(stories)"
   ],
   "id": "392cb35751f3621d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1100\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.629548Z",
     "start_time": "2024-11-29T18:25:34.626564Z"
    }
   },
   "cell_type": "code",
   "source": "stories[0]",
   "id": "7b16b9c250ad95ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.647027Z",
     "start_time": "2024-11-29T18:25:34.644990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(stories), batch_size):\n",
    "        yield stories[i : i + batch_size][\"text\"]"
   ],
   "id": "3194dbbb94bb136f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Tokenizer",
   "id": "f851c79cd10b1e18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.903680Z",
     "start_time": "2024-11-29T18:25:34.652337Z"
    }
   },
   "cell_type": "code",
   "source": "base_tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen2.5-0.5B\", errors=\"ignore\")",
   "id": "8688eae39836a14b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:34.919080Z",
     "start_time": "2024-11-29T18:25:34.916992Z"
    }
   },
   "cell_type": "code",
   "source": "base_tokenizer.decode([51461], errors=\"replace\")",
   "id": "7a59bd82892ad6d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ï¿½'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.221985Z",
     "start_time": "2024-11-29T18:25:34.930895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 1024\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=vocab_size\n",
    ")"
   ],
   "id": "f569b198e7f04743",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test untrained model",
   "id": "253b7be310f9fb50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize model",
   "id": "ad5eb0b5c81c4950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.235021Z",
     "start_time": "2024-11-29T18:25:35.232366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 64\n",
    "\n",
    "config = Qwen2Config(\n",
    "    num_hidden_layers=3,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size * 4,  # MLP hidden dim, following GPT-2 approach x4\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=2, # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=512,  # Maximum sequence length\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "config"
   ],
   "id": "4fd74421ca7947f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 1024\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.251860Z",
     "start_time": "2024-11-29T18:25:35.245225Z"
    }
   },
   "cell_type": "code",
   "source": "model = Qwen2ForCausalLM(config)",
   "id": "456dba1060c3b4c3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.263740Z",
     "start_time": "2024-11-29T18:25:35.261873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "id": "de9a72e416747101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 309,984\n",
      "Total size of the model: 1.18 MB\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate text from the model",
   "id": "c25f0110366ebb96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.275768Z",
     "start_time": "2024-11-29T18:25:35.273940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_tokens_to_dataframe(tokenizer, inputs):\n",
    "    \"\"\"Decodes token IDs to tokens and returns them as a transposed DataFrame.\"\"\"\n",
    "    decoded_tokens = [tokenizer.decode(token_id) for token_id in inputs[0]]\n",
    "    token_ids = inputs[0].tolist()\n",
    "\n",
    "    # Create and return a transposed DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Token\": decoded_tokens,\n",
    "        \"Token ID\": token_ids\n",
    "    })\n",
    "\n",
    "    return df.T"
   ],
   "id": "a057ffc91dde7302",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.286279Z",
     "start_time": "2024-11-29T18:25:35.284700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"One day a little girl, wakanda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ],
   "id": "1be3e7548940ddfa",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.307270Z",
     "start_time": "2024-11-29T18:25:35.298379Z"
    }
   },
   "cell_type": "code",
   "source": "decode_tokens_to_dataframe(tokenizer, inputs[\"input_ids\"])",
   "id": "591e96099b3edbd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0     1    2        3      4   5    6   7    8   9\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a\n",
       "Token ID  446   371  272      406    451  25  283  88  711  78"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.326167Z",
     "start_time": "2024-11-29T18:25:35.318878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=16\n",
    ")"
   ],
   "id": "c2c34dca9c60cd80",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.353224Z",
     "start_time": "2024-11-29T18:25:35.347270Z"
    }
   },
   "cell_type": "code",
   "source": "decode_tokens_to_dataframe(tokenizer, outputs)",
   "id": "1ffbbc542101f0c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0     1    2        3      4   5    6   7    8   9     10   11  \\\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a   has  ice   \n",
       "Token ID  446   371  272      406    451  25  283  88  711  78   996  528   \n",
       "\n",
       "            12   13  14  15  \n",
       "Token     very   lo   e   m  \n",
       "Token ID   363  464  82  90  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>has</td>\n",
       "      <td>ice</td>\n",
       "      <td>very</td>\n",
       "      <td>lo</td>\n",
       "      <td>e</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "      <td>996</td>\n",
       "      <td>528</td>\n",
       "      <td>363</td>\n",
       "      <td>464</td>\n",
       "      <td>82</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.392667Z",
     "start_time": "2024-11-29T18:25:35.390396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_to_id = tokenizer.get_vocab()\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ],
   "id": "79f4eeb7838fba9e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> From [Qwen/tokenization_note.md](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md): The regular tokens are BPE tokens learned from byte sequences of texts encoded using the UTF-8 encoding. While this allows tokenization of all texts and no unknown token exists, it may fall back to using single bytes when tokenizing uncommon texts. You may encounter UTF-8 decoding errors and as the errors are default to replace, thus the replacement character (ï¿½) in incomplete generation.",
   "id": "925ccb00839f049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.484152Z",
     "start_time": "2024-11-29T18:25:35.481627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(id_to_token[189])\n",
    "print(tokenizer.decode(189))\n",
    "print(tokenizer.convert_ids_to_tokens(189))"
   ],
   "id": "d25a32e087b0e93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã³\n",
      "ï¿½\n",
      "Ã³\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.527655Z",
     "start_time": "2024-11-29T18:25:35.525517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(id_to_token[271])\n",
    "print(tokenizer.decode(271))\n",
    "print(tokenizer.convert_ids_to_tokens(271))"
   ],
   "id": "c24a68905a65083f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä t\n",
      " t\n",
      "Ä t\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train language model",
   "id": "b8d4a55ba24573c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create Lightning Data Module\n",
    "\n",
    "In this case, we are creating a simple Lightning module when we have already loaded the Dataset. We would normally want to use this in a more complex manner, like with an iterableDataset from a series of Parquet files"
   ],
   "id": "3ac9ccb01e1708f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:35.560440Z",
     "start_time": "2024-11-29T18:25:35.555898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name: str,\n",
    "            n_train_rows: int,\n",
    "            n_val_rows: int,\n",
    "            batch_size: int,\n",
    "            max_seq_length: int,\n",
    "            num_workers: int,\n",
    "            tokenizer: Qwen2TokenizerFast,\n",
    "            random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset_name: Name of the dataset.\n",
    "        :param n_train_rows: Number of training rows.\n",
    "        :param n_val_rows: Number of validation rows.\n",
    "        :param batch_size: Batch size.\n",
    "        :param max_seq_length: Max sequence length.\n",
    "        :param num_workers: Number of workers.\n",
    "        :param random_seed: Random seed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.n_train_rows = n_train_rows\n",
    "        self.n_val_rows = n_val_rows\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Load dataset in streaming mode\n",
    "        ds = load_dataset(\n",
    "            self.dataset_name,\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Create dataset\n",
    "        self.train_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"train\",\n",
    "            n_rows=self.n_train_rows,\n",
    "        )\n",
    "        self.val_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"validation\",\n",
    "            n_rows=self.n_val_rows,\n",
    "        )\n",
    "\n",
    "        # Tokenizer\n",
    "        # TODO: In reality, we would the tokenizer here\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _create_dataset(self, ds, split, n_rows):\n",
    "        # Download and load N rows\n",
    "        rows = list(stories_stream[split].take(n_rows))\n",
    "        return Dataset.from_list(rows)\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "\n",
    "        batch_text = [item[\"text\"] for item in batch]\n",
    "\n",
    "        # Tokenize texts and stack the resulting tensors\n",
    "        batch_tokenized = self.tokenizer(\n",
    "            batch_text,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return batch_tokenized"
   ],
   "id": "f0208ba94b9df4a4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test the `DataModule`",
   "id": "656842cb385e09aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:37.883266Z",
     "start_time": "2024-11-29T18:25:35.577377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module = DataModule(\n",
    "    dataset_name='roneneldan/TinyStories',  # The dataset name\n",
    "    n_train_rows=10,                       # For testing, load only 10 rows\n",
    "    n_val_rows=10,                         # For testing, load only 10 validation rows\n",
    "    batch_size=2,                          # Smaller batch size for testing\n",
    "    max_seq_length=128,                    # Choose a max sequence length (e.g., 128)\n",
    "    num_workers=0,                         # No need for multiple workers in a test scenario\n",
    "    tokenizer=tokenizer,                   # Pass the tokenizer here\n",
    "    random_seed=42                          # Optional, for reproducibility\n",
    ")\n",
    "\n",
    "# Set up the data module\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "# Get the first batch from the train dataloader\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "\n",
    "# Iterate over the dataloader to get the first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the first batch to inspect it\n",
    "print(first_batch)"
   ],
   "id": "c359a5af3a412b0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[446, 371,  25, 272, 406, 451, 572, 398, 616, 272, 831, 322, 333, 326,\n",
      "         758,  27, 325, 789, 318, 294, 291, 602,  83, 477,  98,  89,  97, 280,\n",
      "         377, 353, 318, 883, 318, 294, 391, 301,  93,  27, 398, 461, 280, 902,\n",
      "         276, 831, 322, 353, 326, 399,  25, 356, 348, 480, 430, 100, 272, 433,\n",
      "          97, 299, 361, 326, 391, 327,  97, 319, 370, 491, 280, 326, 399, 278,\n",
      "         343,  25, 350, 810,  25, 346, 616, 745, 831, 322,  27, 880, 314, 366,\n",
      "         902, 318, 353, 516, 278, 430, 100, 625, 391, 327,  97, 610, 900, 399,\n",
      "         508, 278, 343,  25, 350, 922,  25, 398,  25, 373, 481, 902, 276, 831,\n",
      "         322, 278, 966, 101, 637, 391, 327,  97, 503,  65,  92, 576,  25, 380,\n",
      "         391, 606],\n",
      "        [455, 475, 272, 420,  25, 424, 294, 272, 406, 569, 572, 382,  82, 580,\n",
      "          27, 382,  82, 580, 560, 280, 449, 848, 278, 377, 333, 276, 749,  27,\n",
      "         382,  82, 580, 294, 272, 292, 416,  97,  85, 102, 569, 883, 292, 683,\n",
      "         375, 614, 285,  98, 379,  27, 913, 499, 285,  98, 379, 573, 382,  82,\n",
      "         580, 427, 278, 984, 319, 446, 371,  25, 382,  82, 580, 294, 291, 365,\n",
      "         777, 333, 276, 633, 593, 292, 434, 272, 425, 738,  27, 309, 738, 375,\n",
      "         785, 505,  78, 796, 372, 438, 285, 403, 303,  27, 382,  82, 580, 638,\n",
      "         766, 276, 505,  78, 796, 285, 403, 278, 461, 280, 377, 353, 452,  27,\n",
      "         382,  82, 580, 291, 649, 354, 958, 276, 738, 278, 739, 889, 276, 505,\n",
      "          78, 796]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create ModelModule\n",
    "\n",
    "#### Shifting Inputs and Labels\n",
    "\n",
    "In language model pretraining, the goal is to predict the next token given the previous tokens. This is achieved by shifting the input and labels:\n",
    "\n",
    "- **Labels Shift**: `labels[..., 1:]` means \"take all dimensions, but slice off the first token from the last dimension.\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **Original input**: `[1, 2, 3, 4]`\n",
    "- **Shifted input**: `[2, 3, 4]`\n",
    "\n",
    "This creates an offset, ensuring that each input token predicts the next token.\n",
    "\n",
    "#### Why Shifting?\n",
    "\n",
    "In language modeling, the objective is to predict the next token. Therefore, the input `[1, 2, 3]` should predict the next tokens `[2, 3, 4]`. This is distinct from tasks like classification, where labels match exactly.\n",
    "\n",
    "**Example to Illustrate:**\n",
    "\n",
    "- **Input**: `[start, \"I\", \"love\", \"machine\"]`\n",
    "- **Labels**: `[\"I\", \"love\", \"machine\", \"learning\"]`\n",
    "- **Shifted Input**: `[\"I\", \"love\", \"machine\"]`\n",
    "- **Shifted Labels**: `[\"love\", \"machine\", \"learning\"]`\n",
    "\n",
    "#### Loss Computation\n",
    "\n",
    "The `CrossEntropyLoss` combines `log softmax` and `NLL (Negative Log Likelihood)` loss. The `reduction='mean'` option averages the loss across all tokens.\n",
    "\n",
    "- **Logits Transformation**: `logits.view(-1, logits.size(-1))` flattens the logits to a 2D tensor of shape `(batch_size * sequence_length, vocab_size)`.\n",
    "- **Labels Transformation**: `shift_labels.view(-1)` flattens the labels to a 1D tensor of shape `(batch_size * sequence_length)`.\n",
    "\n",
    "This setup ensures that the model is trained to predict the next token in the sequence accurately."
   ],
   "id": "21ed047626d7c594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:37.911517Z",
     "start_time": "2024-11-29T18:25:37.905398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Make it general, not focused around Qwen\n",
    "\n",
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        qwen_model_config: dict,\n",
    "        learning_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = Qwen2ForCausalLM(qwen_model_config)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs.logits\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Shift labels for next-token prediction\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_input_ids = input_ids[..., 1:].contiguous()\n",
    "        shift_attention_mask = attention_mask[..., 1:].contiguous()\n",
    "\n",
    "        # Get logits\n",
    "        logits = self(shift_input_ids, shift_attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=50  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        return generated_ids\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box usually works fine\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)"
   ],
   "id": "8da0a8c8a12e2bb8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:37.934537Z",
     "start_time": "2024-11-29T18:25:37.926174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 64\n",
    "\n",
    "config = Qwen2Config(\n",
    "    num_hidden_layers=3,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size * 4,  # MLP hidden dim, following GPT-2 approach x4\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=2, # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=512,  # Maximum sequence length\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "model_module = ModelModule(\n",
    "    qwen_model_config=config,\n",
    "    learning_rate=1e-5,\n",
    ")"
   ],
   "id": "57ebd9d15e59cad",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:25:37.951610Z",
     "start_time": "2024-11-29T18:25:37.948961Z"
    }
   },
   "cell_type": "code",
   "source": "model_module",
   "id": "cfa5ded8883759fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelModule(\n",
       "  (model): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(1024, 64)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (k_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (up_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (down_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((64,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=64, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up Trainer",
   "id": "d45801af8da08837"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:28:12.111315Z",
     "start_time": "2024-11-29T18:28:12.104227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_trainer():\n",
    "    # Set up callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='./model_checkpoints',  # Directory to save checkpoints\n",
    "        filename='qwen-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,  # Save the top 3 models according to the monitored quantity\n",
    "        verbose=True,\n",
    "        monitor='val_loss',  # Metric to monitor\n",
    "        mode='min'  # We want to minimize the validation loss\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0.00,\n",
    "        patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=False,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    # Set up logger\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir='./tb_logs',\n",
    "        name='qwen-pretrain-logs'\n",
    "    )\n",
    "\n",
    "    # Create the trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,  # Number of training epochs\n",
    "        accelerator='auto',  # Automatically use GPU if available\n",
    "        devices=1,  # Use 1 GPU or CPU\n",
    "        precision=\"16-mixed\",  # Use mixed precision training\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        logger=logger,\n",
    "        fast_dev_run=False,  # Set to True for a quick test run\n",
    "        gradient_clip_val=1.0,  # Gradient clipping\n",
    "        deterministic=True  # For reproducibility\n",
    "    )\n",
    "\n",
    "    return trainer"
   ],
   "id": "76f8c89622cd2681",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:28:16.292808Z",
     "start_time": "2024-11-29T18:28:13.945279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Create model and data modules (as defined in previous context)\n",
    "model_module = ModelModule(\n",
    "    qwen_model_config=config,\n",
    "    learning_rate=1e-5,\n",
    ")\n",
    "\n",
    "data_module = DataModule(\n",
    "    dataset_name='roneneldan/TinyStories',\n",
    "    n_train_rows=10,\n",
    "    n_val_rows=10,\n",
    "    batch_size=2,\n",
    "    max_seq_length=128,\n",
    "    num_workers=0,\n",
    "    tokenizer=tokenizer,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Setup the data module\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "# Create trainer\n",
    "trainer = setup_trainer()"
   ],
   "id": "86a87c9f76b6887c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run training",
   "id": "21c92129d8ded941"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T18:28:28.856548Z",
     "start_time": "2024-11-29T18:28:25.048301Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Fit the model\n",
    "trainer.fit(\n",
    "    model=model_module,\n",
    "    datamodule=data_module\n",
    ")\n"
   ],
   "id": "b178ad67ad2e1806",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernando/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | Qwen2ForCausalLM | 309 K  | train\n",
      "---------------------------------------------------\n",
      "309 K     Trainable params\n",
      "0         Non-trainable params\n",
      "309 K     Total params\n",
      "1.240     Total estimated model params size (MB)\n",
      "49        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71910328ba2945e8a4c1000340f93d36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernando/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/fernando/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Fit the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m   \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m   \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_module\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    570\u001B[0m     ckpt_path,\n\u001B[1;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m )\n\u001B[0;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1023\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m   1025\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1049\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1052\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1054\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1056\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    176\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    395\u001B[0m )\n\u001B[0;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:411\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 411\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 49\u001B[0m, in \u001B[0;36mModelModule.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m---> 49\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommon_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "Cell \u001B[0;32mIn[23], line 24\u001B[0m, in \u001B[0;36mModelModule.common_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     22\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     23\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m---> 24\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlabels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Shift labels for next-token prediction\u001B[39;00m\n\u001B[1;32m     27\u001B[0m shift_labels \u001B[38;5;241m=\u001B[39m labels[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "File \u001B[0;32m~/Documents/GitHub/tiny-lm/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:270\u001B[0m, in \u001B[0;36mBatchEncoding.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03metc.).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;124;03mwith the constraint of slice.\u001B[39;00m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 270\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings[item]\n",
      "\u001B[0;31mKeyError\u001B[0m: 'labels'"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
