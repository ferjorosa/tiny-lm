#name: "roneneldan/TinyStories"
#dataset_text_col: "text"

# Experiment name
name: "tiny_qwen"

# Training Datasets
dataset_name: "Trelis/tiny-shakespeare"
dataset_text_col: "Text"
dataset_length_train: 472
dataset_length_val: 0

# Tokenizer
tokenizer_dir: "tokenizer"
vocab_size: 1024
tokenizer_batch_size: 128

# Model hyperparameters
base_model: "Qwen/Qwen2.5-0.5B"
num_hidden_layers: 1
hidden_size: 64
num_attention_heads: 8
num_key_value_heads: 2 # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA
max_position_embeddings: 512  # Maximum sequence length
attention_probs_dropout_prob: 0.1

# Learning parameters
max_seq_length: 2048
batch_size: 32
learning_rate: !!float 5e-5
precision: "16-mixed"
num_epochs: 1
compute_accelerator: "cpu"
compute_devices: 1
num_workers: 1

logs_dir: "logs"