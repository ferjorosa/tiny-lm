{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42855b390ebff86c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.058002Z",
     "start_time": "2024-11-24T22:53:02.040003Z"
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from src.model.utils.generation import generate_text_simple\n",
    "from src.model.zoo.my_gpt import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.106782Z",
     "start_time": "2024-11-24T22:53:02.104750Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Traditional GPT-2 architecture\n",
    "GPT2_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "    \"emb_drop_rate\": 0.1,  # Embedding dropout rate\n",
    "    \"attn_drop_rate\": 0.1,  # Attention dropout rate\n",
    "    \"res_drop_rate\": 0.1,  # Residual connection dropout rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b14633947ac9a727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.743861Z",
     "start_time": "2024-11-24T22:53:02.110563Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT2_CONFIG_124M)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd867ac90824d7b4",
   "metadata": {},
   "source": "## Prepare data"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6bf781f1634af20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.759115Z",
     "start_time": "2024-11-24T22:53:02.750301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a38f4283e5707",
   "metadata": {},
   "source": "## Run batch"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c142bb67bf490d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.888088Z",
     "start_time": "2024-11-24T22:53:02.766469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4efbbf601c9b53",
   "metadata": {},
   "source": "As we can see the output tensor has the shape `[2, 4, 50257]`, since we passed in tow input texts with four tokens each. The last dimension, `50257`, corresponds to the vocabulary size."
  },
  {
   "cell_type": "markdown",
   "id": "cbcfc4b004ab2133",
   "metadata": {},
   "source": "## Check parameter size"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c0e43b048435fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.897173Z",
     "start_time": "2024-11-24T22:53:02.894872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fb6f2d998133e",
   "metadata": {},
   "source": [
    "As can be seen, there is a slight discrepancy. We say this is a 124M parameter GPT model, so why i is the actual number of parameters 163 million?\n",
    "\n",
    "The reason is a concept called weight tying, which was used in the original GPT-2 architecture. It means that the original GPT-2 architecture reuses the weights from\n",
    "the token embedding layer in its output layer. To understand better, letâ€™s take a look at the shapes of the token embedding layer and linear output layer that we initialized on the model via the GPTModel earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27dc9d8afe50a898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.907624Z",
     "start_time": "2024-11-24T22:53:02.905463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014e48ef2b8db56",
   "metadata": {},
   "source": [
    "Weight tying reduces the overall memory footprint and computational complexity of the model. However, it may result in worse performance. In his book, Sebastian Raschka comments that using separate token embedding and output layers results in better training and model performance; hence, we use separate layers in our GPTModel implementation.\n",
    "\n",
    "However, the original GPT-2 implementation uses weight-tying and it is necessary if we want to fully \"reuse\" this model for loading pre-trained weights from GPT-2.\n",
    "\n",
    "Other models that use weight-tying are:\n",
    "\n",
    "* [Mobile-LLM](https://arxiv.org/abs/2402.14905)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ed9b8d181feed",
   "metadata": {},
   "source": "## Generate text"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf0e412f5de16eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:02.920432Z",
     "start_time": "2024-11-24T22:53:02.916290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7d5141bbf77557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:03.157446Z",
     "start_time": "2024-11-24T22:53:02.926961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT2_CONFIG_124M[\"context_length\"],\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f777bd9ff6366f",
   "metadata": {},
   "source": "Remove the batch dimension and convert it back into text:"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e517b97e916d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T22:53:03.229987Z",
     "start_time": "2024-11-24T22:53:03.225805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8758ab42f9724b9",
   "metadata": {},
   "source": "Notice that if the model is untrained the outputs will be random"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
