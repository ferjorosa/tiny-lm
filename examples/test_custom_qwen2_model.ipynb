{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:31.059773Z",
     "start_time": "2024-11-27T22:43:08.978109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import Qwen2Config, Qwen2TokenizerFast, Qwen2ForCausalLM"
   ],
   "id": "17474af572a0fc4a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "f3a9880ae995fefa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:34.282455Z",
     "start_time": "2024-11-27T22:43:31.075660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset in streaming mode\n",
    "stories_stream = load_dataset('roneneldan/TinyStories', streaming=True, trust_remote_code=True)\n",
    "stories_stream"
   ],
   "id": "13802a548b974b13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 4\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['text'],\n",
       "        num_shards: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:35.399953Z",
     "start_time": "2024-11-27T22:43:34.675222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_rows = 1100\n",
    "\n",
    "# Get the first 100 rows\n",
    "rows = list(stories_stream['train'].take(n_rows))\n",
    "\n",
    "# Count the total number of characters\n",
    "total_chars = sum(len(row[\"text\"]) for row in rows)\n",
    "total_chars"
   ],
   "id": "977f99d006615c3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:35.426880Z",
     "start_time": "2024-11-27T22:43:35.405290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stories = Dataset.from_list(rows)\n",
    "\n",
    "print(stories)"
   ],
   "id": "392cb35751f3621d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1100\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:35.434021Z",
     "start_time": "2024-11-27T22:43:35.432170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(stories), batch_size):\n",
    "        yield stories[i : i + batch_size][\"text\"]"
   ],
   "id": "3194dbbb94bb136f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Tokenizer",
   "id": "f851c79cd10b1e18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:35.742830Z",
     "start_time": "2024-11-27T22:43:35.440455Z"
    }
   },
   "cell_type": "code",
   "source": "base_tokenizer = Qwen2TokenizerFast.from_pretrained(\"Qwen/Qwen2.5-0.5B\", errors=\"ignore\")",
   "id": "8688eae39836a14b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:35.762218Z",
     "start_time": "2024-11-27T22:43:35.759299Z"
    }
   },
   "cell_type": "code",
   "source": "base_tokenizer.decode([51461], errors=\"replace\")",
   "id": "7a59bd82892ad6d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ï¿½'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.075900Z",
     "start_time": "2024-11-27T22:43:35.783069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 1024\n",
    "tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=vocab_size\n",
    ")"
   ],
   "id": "f569b198e7f04743",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test untrained model",
   "id": "253b7be310f9fb50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize model",
   "id": "ad5eb0b5c81c4950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.083495Z",
     "start_time": "2024-11-27T22:43:36.080055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_size = 64\n",
    "\n",
    "config = Qwen2Config(\n",
    "    num_hidden_layers=3,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size * 4,  # MLP hidden dim, following GPT-2 approach x4\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=2, # if equal to the num_attention heads, the MHA if 1 then MQA, else GQA\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=512,  # Maximum sequence length\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "config"
   ],
   "id": "4fd74421ca7947f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 1024\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.111784Z",
     "start_time": "2024-11-27T22:43:36.094173Z"
    }
   },
   "cell_type": "code",
   "source": "model = Qwen2ForCausalLM(config)",
   "id": "456dba1060c3b4c3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.123831Z",
     "start_time": "2024-11-27T22:43:36.121902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "id": "de9a72e416747101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 309,984\n",
      "Total size of the model: 1.18 MB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Generate text from the model",
   "id": "c25f0110366ebb96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.136014Z",
     "start_time": "2024-11-27T22:43:36.134157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_tokens_to_dataframe(tokenizer, inputs):\n",
    "    \"\"\"Decodes token IDs to tokens and returns them as a transposed DataFrame.\"\"\"\n",
    "    decoded_tokens = [tokenizer.decode(token_id) for token_id in inputs[0]]\n",
    "    token_ids = inputs[0].tolist()\n",
    "\n",
    "    # Create and return a transposed DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Token\": decoded_tokens,\n",
    "        \"Token ID\": token_ids\n",
    "    })\n",
    "\n",
    "    return df.T"
   ],
   "id": "a057ffc91dde7302",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.147832Z",
     "start_time": "2024-11-27T22:43:36.145797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"One day a little girl, wakanda\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ],
   "id": "1be3e7548940ddfa",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.212209Z",
     "start_time": "2024-11-27T22:43:36.175137Z"
    }
   },
   "cell_type": "code",
   "source": "decode_tokens_to_dataframe(tokenizer, inputs[\"input_ids\"])",
   "id": "591e96099b3edbd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0     1    2        3      4   5    6   7    8   9\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a\n",
       "Token ID  446   371  272      406    451  25  283  88  711  78"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.255972Z",
     "start_time": "2024-11-27T22:43:36.225204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=16\n",
    ")"
   ],
   "id": "c2c34dca9c60cd80",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.284439Z",
     "start_time": "2024-11-27T22:43:36.277157Z"
    }
   },
   "cell_type": "code",
   "source": "decode_tokens_to_dataframe(tokenizer, outputs)",
   "id": "1ffbbc542101f0c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           0     1    2        3      4   5    6   7    8   9   10   11   12  \\\n",
       "Token     One   day    a   little   girl   ,   wa   k  and   a   G    ï¿½   li   \n",
       "Token ID  446   371  272      406    451  25  283  88  711  78  52  161  383   \n",
       "\n",
       "          13   14    15  \n",
       "Token      b    ï¿½   out  \n",
       "Token ID  79  127   466  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token</th>\n",
       "      <td>One</td>\n",
       "      <td>day</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "      <td>girl</td>\n",
       "      <td>,</td>\n",
       "      <td>wa</td>\n",
       "      <td>k</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>G</td>\n",
       "      <td>ï¿½</td>\n",
       "      <td>li</td>\n",
       "      <td>b</td>\n",
       "      <td>ï¿½</td>\n",
       "      <td>out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>446</td>\n",
       "      <td>371</td>\n",
       "      <td>272</td>\n",
       "      <td>406</td>\n",
       "      <td>451</td>\n",
       "      <td>25</td>\n",
       "      <td>283</td>\n",
       "      <td>88</td>\n",
       "      <td>711</td>\n",
       "      <td>78</td>\n",
       "      <td>52</td>\n",
       "      <td>161</td>\n",
       "      <td>383</td>\n",
       "      <td>79</td>\n",
       "      <td>127</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.324931Z",
     "start_time": "2024-11-27T22:43:36.320129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_to_id = tokenizer.get_vocab()\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ],
   "id": "79f4eeb7838fba9e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> From [Qwen/tokenization_note.md](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md): The regular tokens are BPE tokens learned from byte sequences of texts encoded using the UTF-8 encoding. While this allows tokenization of all texts and no unknown token exists, it may fall back to using single bytes when tokenizing uncommon texts. You may encounter UTF-8 decoding errors and as the errors are default to replace, thus the replacement character (ï¿½) in incomplete generation.",
   "id": "925ccb00839f049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:43:36.395390Z",
     "start_time": "2024-11-27T22:43:36.392865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(id_to_token[189])\n",
    "print(tokenizer.decode(189))\n",
    "print(tokenizer.convert_ids_to_tokens(189))"
   ],
   "id": "d25a32e087b0e93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã³\n",
      "ï¿½\n",
      "Ã³\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T20:22:35.053461Z",
     "start_time": "2024-11-27T20:22:35.045987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(id_to_token[271])\n",
    "print(tokenizer.decode(271))\n",
    "print(tokenizer.convert_ids_to_tokens(271))"
   ],
   "id": "c24a68905a65083f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä t\n",
      " t\n",
      "Ä t\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train language model",
   "id": "b8d4a55ba24573c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create Lightning Data Module\n",
    "\n",
    "In this case, we are creating a simple Lightning module when we have already loaded the Dataset. We would normally want to use this in a more complex manner, like with an iterableDataset from a series of Parquet files"
   ],
   "id": "3ac9ccb01e1708f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:50:07.894985Z",
     "start_time": "2024-11-27T22:50:07.882423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_name: str,\n",
    "            n_train_rows: int,\n",
    "            n_val_rows: int,\n",
    "            batch_size: int,\n",
    "            max_seq_length: int,\n",
    "            num_workers: int,\n",
    "            tokenizer: Qwen2TokenizerFast,\n",
    "            random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param dataset_name: Name of the dataset.\n",
    "        :param n_train_rows: Number of training rows.\n",
    "        :param n_val_rows: Number of validation rows.\n",
    "        :param batch_size: Batch size.\n",
    "        :param max_seq_length: Max sequence length.\n",
    "        :param num_workers: Number of workers.\n",
    "        :param random_seed: Random seed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.n_train_rows = n_train_rows\n",
    "        self.n_val_rows = n_val_rows\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_workers = num_workers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Load dataset in streaming mode\n",
    "        ds = load_dataset(\n",
    "            self.dataset_name,\n",
    "            streaming=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Create dataset\n",
    "        self.train_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"train\",\n",
    "            n_rows=self.n_train_rows,\n",
    "        )\n",
    "        self.val_ds = self._create_dataset(\n",
    "            ds=ds,\n",
    "            split=\"validation\",\n",
    "            n_rows=self.n_val_rows,\n",
    "        )\n",
    "\n",
    "        # Tokenizer\n",
    "        # TODO: In reality, we would the tokenizer here\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=self._collate_batch,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _create_dataset(self, ds, split, n_rows):\n",
    "        # Download and load N rows\n",
    "        rows = list(stories_stream[split].take(n_rows))\n",
    "        return Dataset.from_list(rows)\n",
    "\n",
    "    def _collate_batch(self, batch):\n",
    "\n",
    "        batch_text = [item[\"text\"] for item in batch]\n",
    "\n",
    "        # Tokenize texts and stack the resulting tensors\n",
    "        batch_tokenized = self.tokenizer(\n",
    "            batch_text,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return batch_tokenized"
   ],
   "id": "f0208ba94b9df4a4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test the `DataModule`",
   "id": "656842cb385e09aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T22:50:13.941783Z",
     "start_time": "2024-11-27T22:50:11.058223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_module = DataModule(\n",
    "    dataset_name='roneneldan/TinyStories',  # The dataset name\n",
    "    n_train_rows=10,                       # For testing, load only 10 rows\n",
    "    n_val_rows=10,                         # For testing, load only 10 validation rows\n",
    "    batch_size=2,                          # Smaller batch size for testing\n",
    "    max_seq_length=128,                    # Choose a max sequence length (e.g., 128)\n",
    "    num_workers=0,                         # No need for multiple workers in a test scenario\n",
    "    tokenizer=tokenizer,                   # Pass the tokenizer here\n",
    "    random_seed=42                          # Optional, for reproducibility\n",
    ")\n",
    "\n",
    "# Set up the data module\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "# Get the first batch from the train dataloader\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "\n",
    "# Iterate over the dataloader to get the first batch\n",
    "first_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the first batch to inspect it\n",
    "print(first_batch)"
   ],
   "id": "c359a5af3a412b0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[446, 371,  25, 272, 406, 451, 572, 398, 616, 272, 831, 322, 333, 326,\n",
      "         758,  27, 325, 789, 318, 294, 291, 602,  83, 477,  98,  89,  97, 280,\n",
      "         377, 353, 318, 883, 318, 294, 391, 301,  93,  27, 398, 461, 280, 902,\n",
      "         276, 831, 322, 353, 326, 399,  25, 356, 348, 480, 430, 100, 272, 433,\n",
      "          97, 299, 361, 326, 391, 327,  97, 319, 370, 491, 280, 326, 399, 278,\n",
      "         343,  25, 350, 810,  25, 346, 616, 745, 831, 322,  27, 880, 314, 366,\n",
      "         902, 318, 353, 516, 278, 430, 100, 625, 391, 327,  97, 610, 900, 399,\n",
      "         508, 278, 343,  25, 350, 922,  25, 398,  25, 373, 481, 902, 276, 831,\n",
      "         322, 278, 966, 101, 637, 391, 327,  97, 503,  65,  92, 576,  25, 380,\n",
      "         391, 606],\n",
      "        [455, 475, 272, 420,  25, 424, 294, 272, 406, 569, 572, 382,  82, 580,\n",
      "          27, 382,  82, 580, 560, 280, 449, 848, 278, 377, 333, 276, 749,  27,\n",
      "         382,  82, 580, 294, 272, 292, 416,  97,  85, 102, 569, 883, 292, 683,\n",
      "         375, 614, 285,  98, 379,  27, 913, 499, 285,  98, 379, 573, 382,  82,\n",
      "         580, 427, 278, 984, 319, 446, 371,  25, 382,  82, 580, 294, 291, 365,\n",
      "         777, 333, 276, 633, 593, 292, 434, 272, 425, 738,  27, 309, 738, 375,\n",
      "         785, 505,  78, 796, 372, 438, 285, 403, 303,  27, 382,  82, 580, 638,\n",
      "         766, 276, 505,  78, 796, 285, 403, 278, 461, 280, 377, 353, 452,  27,\n",
      "         382,  82, 580, 291, 649, 354, 958, 276, 738, 278, 739, 889, 276, 505,\n",
      "          78, 796]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create `ModelModule`",
   "id": "21ed047626d7c594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8da0a8c8a12e2bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run training",
   "id": "d45801af8da08837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76f8c89622cd2681"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
