# GPT-2 tiny (2-layer) with 8k vocab for TinyStories
vocab_size: 8192
context_length: 1024
d_model: 768
n_layers: 2
n_heads: 4
d_ff: 3072
dropout: 0.1
emb_dropout: 0.1
attn_dropout: 0.1
resid_dropout: 0.1
