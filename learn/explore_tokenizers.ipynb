{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5a07b5f8e75c25",
   "metadata": {},
   "source": [
    "* [How to train your tokenizer from scratch using SentencePiece](https://colab.research.google.com/drive/1Ica34BAGK2tuIeQl01SRNTjujPq5C3d1?usp=sharing#scrollTo=x_9uWrrD1GUD)\n",
    "* [HuggingFace example on how to train tokenizers](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb#scrollTo=tQ-nlKovypyv)\n",
    "\n",
    "In this notebook, we will follow Huggingface's guidance to train a new tokenizer using an existing one. This approach allows us to leverage the predefined rules and optimizations of the original tokenizer.\n",
    "\n",
    "Specifically, weâ€™ll work with the Qwen tokenizer, which is built on tiktoken. Its functionality is quite similar to that of GPT-2, with only minor differences. We'll explore those differences more thoroughly another time.\n",
    "\n",
    "> **Note:** These days, most large language models (LLMs) rely on Byte Pair Encoding (BPE) and utilize tiktoken for tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77254eaf-211c-41dc-ae21-038d2692b1af",
   "metadata": {},
   "source": [
    "## 1 - Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31b2e8e-d588-43dd-b1b0-a394a8a16503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014715"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset in streaming mode\n",
    "stories_stream = load_dataset(\n",
    "    \"roneneldan/TinyStories\", streaming=True, trust_remote_code=True\n",
    ")\n",
    "\n",
    "n_rows = 1100\n",
    "\n",
    "# Get the first 100 rows\n",
    "rows = list(stories_stream[\"train\"].take(n_rows))\n",
    "\n",
    "# Count the total number of characters\n",
    "total_chars = sum(len(row[\"text\"]) for row in rows)\n",
    "total_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814e88f-9c29-4953-ae36-98c282dfa14b",
   "metadata": {},
   "source": [
    "Make it a HuggingFace dataset, for simplicity sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f58ddf-31dc-43bc-8ddb-a3736fa4fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "stories = Dataset.from_list(rows)\n",
    "\n",
    "print(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36592a52-710b-4673-8eb6-ed76834b4597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2131594-dda2-4b72-94b8-4c22ff0aac12",
   "metadata": {},
   "source": [
    "We are also going to use a batch_iterator, as if it were a big dataset. For learning sake, we are going to use a small `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31dd95ab-9b92-44be-9b8f-3df386f9cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(stories), batch_size):\n",
    "        yield stories[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c435abdb-b630-48b1-9ccb-537dbefad501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the batches and print the first few items of each batch\n",
    "for batch_number, batch in enumerate(batch_iterator(), start=1):\n",
    "    print(f\"Batch {batch_number} (Size {len(batch)}):\")\n",
    "    print(batch[0])  # show just the first instance of the batch\n",
    "    print(\"-\" * 40)  # Separator between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4b901-d531-4089-bae0-3a1dc6d47fbe",
   "metadata": {},
   "source": [
    "## 2 - Train tokenizer using GPT-2 / Qwen tokenization algorithms\n",
    "\n",
    "If we want to train a tokenizer with the exact same algorithms and parameters as an existing one, we can just use the `train_new_from_iterator` API from ðŸ¤— Tokenizers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f39969b5-aa07-42a4-b268-41022165b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f57bdca-bd36-4263-a119-a84779d5716e",
   "metadata": {},
   "source": [
    "Make sure that the tokenizer you picked as a *fast* version (backed by the ðŸ¤— Tokenizers library) otherwise the rest of the notebook will not run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee34278a-aa59-478d-b63b-a1a7a7549d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_tokenizer.is_fast)\n",
    "print(qwen_tokenizer.is_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc7a73-d114-42d0-bb3e-73e32728fa03",
   "metadata": {},
   "source": [
    "Then we feed the training corpus (either the list of list or the iterator we defined earlier) to the `train_new_from_iterator` method. We also have to specify the **vocabulary size** we want to use.\n",
    "\n",
    "> Note: If the `vocab_size` we pass is lower than the base vocabulary of the tokenizer (i.e., **257** for GPT-2 and **270** for Qwen), it seems to be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38e14f-7546-4603-822e-4db638c7fc80",
   "metadata": {},
   "source": [
    "### 2.1 - Train GPT-2 tokenizer with 1 extra token\n",
    "\n",
    "Let's train the GPT-2 tokenizer with vocab_size of 258. This will mean that we only add 1 new token to the vocabulary after \"training\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39b47d28-70c1-4739-b558-1f5c7b306408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=258, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_gpt2 = gpt2_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=258\n",
    ")\n",
    "new_tokenizer_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8cd5970c-eed8-46c7-bbeb-ce060ce1913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_gpt2.convert_ids_to_tokens(257)  # The new token after BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0010a-7198-4c36-8b45-e5fe621b9ed9",
   "metadata": {},
   "source": [
    "### 2.2 - Train Qwen tokenizer with 1 extra token\n",
    "\n",
    "Let's train the  Qwen tokenizer with vocab_size of 271. This will mean that we only add 1 new token to the vocabulary after \"training\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a76286f-346e-4188-9b9a-4bfa8af290d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B', vocab_size=271, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_qwen = qwen_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=271\n",
    ")\n",
    "new_tokenizer_qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b7c70-1331-45fc-b2b3-31174302c5fb",
   "metadata": {},
   "source": [
    "### 2.3.- Train GPT-2 Tokenizer with 1024 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ca9e6d7-a146-4e1a-b67a-970b0a555ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_gpt2 = gpt2_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9ecee-2598-4188-926c-f741c00b9f48",
   "metadata": {},
   "source": [
    "Although we dont know the counts of each token, we can see which tokens were the \"least common\" by their rank in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ea6db1e-f412-43cf-a0bb-1ab37d5eb8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tokens:\n",
      "Token: <|endoftext|>, ID: 0\n",
      "Token: !, ID: 1\n",
      "Token: \", ID: 2\n",
      "Token: #, ID: 3\n",
      "Token: $, ID: 4\n",
      "\n",
      "Bottom 5 tokens:\n",
      "Token: Ä lived, ID: 1019\n",
      "Token: raw, ID: 1020\n",
      "Token: ash, ID: 1021\n",
      "Token: Ä When, ID: 1022\n",
      "Token: so, ID: 1023\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary from the tokenizer\n",
    "vocab_gpt2 = new_tokenizer_gpt2.get_vocab()\n",
    "\n",
    "# Sort the vocabulary by token id (in ascending order)\n",
    "sorted_vocab = sorted(vocab_gpt2.items(), key=lambda x: x[1])\n",
    "\n",
    "number = 5\n",
    "\n",
    "# Print the top token IDs\n",
    "print(f\"Top {number} tokens:\")\n",
    "for token, token_id in sorted_vocab[:number]:\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "\n",
    "# Print the bottom token IDs\n",
    "print(f\"\\nBottom {number} tokens:\")\n",
    "for token, token_id in sorted_vocab[-number:]:\n",
    "    print(f\"Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9edd8-f654-4fbe-b2e1-335f53bbddd8",
   "metadata": {},
   "source": [
    "### 2.4 - Train Qwen tokenizer with 1024 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80a42f27-e6cc-4e94-a323-ce82efde998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_qwen = qwen_tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0af789a4-08ff-4e39-b1c1-1f9d2bcdd615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tokens:\n",
      "Token: <|endoftext|>, ID: 0\n",
      "Token: <|im_start|>, ID: 1\n",
      "Token: <|im_end|>, ID: 2\n",
      "Token: <|object_ref_start|>, ID: 3\n",
      "Token: <|object_ref_end|>, ID: 4\n",
      "\n",
      "Bottom 5 tokens:\n",
      "Token: bbit, ID: 1032\n",
      "Token: Ä pic, ID: 1033\n",
      "Token: Ä ow, ID: 1034\n",
      "Token: Ä watch, ID: 1035\n",
      "Token: Ä brave, ID: 1036\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary from the tokenizer\n",
    "vocab_qwen = new_tokenizer_qwen.get_vocab()\n",
    "\n",
    "# Sort the vocabulary by token id (or you can choose to sort by frequency if needed)\n",
    "# Sorting by token ID in ascending order (if you want the first 5 ids)\n",
    "sorted_vocab_qwen = sorted(vocab_qwen.items(), key=lambda x: x[1])\n",
    "\n",
    "number = 5\n",
    "# Print the top token IDs\n",
    "print(f\"Top {number} tokens:\")\n",
    "for token, token_id in sorted_vocab_qwen[:number]:\n",
    "    print(f\"Token: {token}, ID: {token_id}\")\n",
    "\n",
    "# Print the bottom token IDs\n",
    "print(f\"\\nBottom {number} tokens:\")\n",
    "for token, token_id in sorted_vocab[-number:]:\n",
    "    print(f\"Token: {token}, ID: {token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0797734e-0a7a-4e06-aadd-29ef48714076",
   "metadata": {},
   "source": [
    "The difference in tokens is due to the varying algorithms, as I tested 1024 + 13 tokens and observed that they also yielded different tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea6e67-23ab-441a-a64c-b5c630544851",
   "metadata": {},
   "source": [
    "### 2.5 - Save tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09f54318-cd64-4af2-b8ed-1f355ca85a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('custom-gpt2-tokenizer/tokenizer_config.json',\n",
       " 'custom-gpt2-tokenizer/special_tokens_map.json',\n",
       " 'custom-gpt2-tokenizer/vocab.json',\n",
       " 'custom-gpt2-tokenizer/merges.txt',\n",
       " 'custom-gpt2-tokenizer/added_tokens.json',\n",
       " 'custom-gpt2-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_gpt2.save_pretrained(\"custom-gpt2-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d421f590-3fbd-4c63-b163-bfd156a60853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('custom-qwen-tokenizer/tokenizer_config.json',\n",
       " 'custom-qwen-tokenizer/special_tokens_map.json',\n",
       " 'custom-qwen-tokenizer/vocab.json',\n",
       " 'custom-qwen-tokenizer/merges.txt',\n",
       " 'custom-qwen-tokenizer/added_tokens.json',\n",
       " 'custom-qwen-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_qwen.save_pretrained(\"custom-qwen-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c37f6d-020e-48b5-bc1c-9c7b2326612d",
   "metadata": {},
   "source": [
    "## 3 - Building Our Custom Tokenizer from Scratch\n",
    "\n",
    "If you're interested, here are several helpful resources:\n",
    "\n",
    "* [HuggingFace Notebook Example](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb#scrollTo=9wG8PjQSj-Nc)\n",
    "* [HuggingFace Forum Thread on How to Build a Custom LlamaTokenizer](https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835)\n",
    "    * [Colab Notebook Showing How to Do It with SentencePiece](https://colab.research.google.com/drive/1Ica34BAGK2tuIeQl01SRNTjujPq5C3d1?usp=sharing), then check the thread to see [how to save it as a FastTokenizer](https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835/15) (not sure if this would work, to be honest)\n",
    "* HuggingFace Transformers Library Implementation of Tokenizers:\n",
    "    * [GPT-2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/tokenization_gpt2_fast.py)\n",
    "    * [Qwen](https://github.com/huggingface/transformers/blob/v4.46.3/src/transformers/models/qwen2/tokenization_qwen2_fast.py)\n",
    "* Original Repo Implementations:\n",
    "    * [Llama](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py) (as an example; other models like Qwen are available directly in HuggingFace)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
