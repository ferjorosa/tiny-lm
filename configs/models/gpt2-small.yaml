# GPT-2 small (124M-style, smaller vocab for TinyStories)
vocab_size: 8192
context_length: 1024
d_model: 768
n_layers: 12
n_heads: 12
d_ff: 3072
dropout: 0.1
emb_dropout: 0.1
attn_dropout: 0.1
resid_dropout: 0.1
